{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c67a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 84\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: (precision_mean, precision_ci_low, precision_ci_high),\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: (recall_mean, recall_ci_low, recall_ci_high),\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m'\u001b[39m: (ndcg_mean, ndcg_ci_low, ndcg_ci_high)\n\u001b[1;32m     81\u001b[0m     }\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Evaluate both models\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m cbf_results \u001b[38;5;241m=\u001b[39m metrics_with_ci(preds_cbf, test_df, K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     85\u001b[0m hybrid_results \u001b[38;5;241m=\u001b[39m metrics_with_ci(preds_hybrid, test_df, K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 65\u001b[0m, in \u001b[0;36mmetrics_with_ci\u001b[0;34m(pred_dict, test_df, K, confidence)\u001b[0m\n\u001b[1;32m     63\u001b[0m     precision_samples\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m     64\u001b[0m     recall_samples\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m---> 65\u001b[0m     ndcg_samples\u001b[38;5;241m.\u001b[39mappend(ndcg_at_k({\u001b[38;5;28mstr\u001b[39m(user): pred_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mstr\u001b[39m(user), [])}, user_df, K))\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_ci\u001b[39m(samples):\n\u001b[1;32m     68\u001b[0m     mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(samples)\n",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m, in \u001b[0;36mndcg_at_k\u001b[0;34m(pred_dict, test_df, K)\u001b[0m\n\u001b[1;32m     50\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(rel)\n\u001b[1;32m     51\u001b[0m     truths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msorted\u001b[39m(rel, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ndcg_score(truths, scores)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:1827\u001b[0m, in \u001b[0;36mndcg_score\u001b[0;34m(y_true, y_score, k, sample_weight, ignore_ties)\u001b[0m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1724\u001b[0m     {\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1732\u001b[0m )\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mndcg_score\u001b[39m(y_true, y_score, \u001b[38;5;241m*\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_ties\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Normalized Discounted Cumulative Gain.\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \n\u001b[1;32m   1736\u001b[0m \u001b[38;5;124;03m    Sum the true scores ranked in the order induced by the predicted scores,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;124;03m    0.5...\u001b[39;00m\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1827\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1828\u001b[0m     y_score \u001b[38;5;241m=\u001b[39m check_array(y_score, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1829\u001b[0m     check_consistent_length(y_true, y_score, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1081\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m<\u001b[39m ensure_min_features:\n\u001b[0;32m-> 1081\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1082\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m feature(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_features, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_features, context)\n\u001b[1;32m   1085\u001b[0m         )\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# With an input pandas dataframe or series, we know we can always make the\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# resulting array writeable:\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# - if copy=True, we have already made a copy so it is fine to make the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# for more details about pandas copy-on-write mechanism, that is enabled by\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# default in pandas 3.0.0.dev.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df_or_series(array_orig) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import t\n",
    "\n",
    "# Load your original test set\n",
    "test_df = pd.read_csv('subset_ratings.csv')\n",
    "test_df = test_df[test_df.movieId.notna()]\n",
    "\n",
    "# Load predictions\n",
    "with open(\"predictions/cbf_top10_subset.json\") as f:\n",
    "    preds_cbf = json.load(f)\n",
    "\n",
    "with open(\"predictions/hybrid_top10_subset.json\") as f:\n",
    "    preds_hybrid = json.load(f)\n",
    "\n",
    "# Precision and Recall Calculation\n",
    "def precision_recall_at_k(pred_dict, test_df, K=10):\n",
    "    hit_count, rec_count, rel_count = 0, 0, 0\n",
    "\n",
    "    for u, grp in test_df.groupby('userId'):\n",
    "        u = str(u)\n",
    "        if u not in pred_dict:\n",
    "            continue\n",
    "\n",
    "        true_items = set(grp.movieId)\n",
    "        pred_items = pred_dict[u][:K]\n",
    "\n",
    "        hits = len(true_items.intersection(pred_items))\n",
    "        hit_count += hits\n",
    "        rec_count += K\n",
    "        rel_count += len(true_items)\n",
    "\n",
    "    precision = hit_count / rec_count if rec_count else 0\n",
    "    recall = hit_count / rel_count if rel_count else 0\n",
    "    return precision, recall\n",
    "\n",
    "# NDCG Calculation\n",
    "def ndcg_at_k(pred_dict, test_df, K=10):\n",
    "    scores, truths = [], []\n",
    "    for u, grp in test_df.groupby('userId'):\n",
    "        u = str(u)\n",
    "        if u not in pred_dict:\n",
    "            continue\n",
    "\n",
    "        true_items = set(grp.movieId)\n",
    "        pred_items = pred_dict[u][:K]\n",
    "        rel = [1 if m in true_items else 0 for m in pred_items]\n",
    "        scores.append(rel)\n",
    "        truths.append(sorted(rel, reverse=True))\n",
    "\n",
    "    return ndcg_score(truths, scores)\n",
    "\n",
    "# Calculate metrics and confidence intervals\n",
    "def metrics_with_ci(pred_dict, test_df, K=10, confidence=0.95):\n",
    "    precision_samples, recall_samples, ndcg_samples = [], [], []\n",
    "    user_ids = test_df.userId.unique()\n",
    "\n",
    "    for user in user_ids:\n",
    "        user_df = test_df[test_df.userId == user]\n",
    "        p, r = precision_recall_at_k({str(user): pred_dict.get(str(user), [])}, user_df, K)\n",
    "        precision_samples.append(p)\n",
    "        recall_samples.append(r)\n",
    "        ndcg_samples.append(ndcg_at_k({str(user): pred_dict.get(str(user), [])}, user_df, K))\n",
    "\n",
    "    def calc_ci(samples):\n",
    "        mean = np.mean(samples)\n",
    "        sem = np.std(samples, ddof=1) / np.sqrt(len(samples))\n",
    "        margin = sem * t.ppf((1 + confidence) / 2., len(samples) - 1)\n",
    "        return mean, mean - margin, mean + margin\n",
    "\n",
    "    precision_mean, precision_ci_low, precision_ci_high = calc_ci(precision_samples)\n",
    "    recall_mean, recall_ci_low, recall_ci_high = calc_ci(recall_samples)\n",
    "    ndcg_mean, ndcg_ci_low, ndcg_ci_high = calc_ci(ndcg_samples)\n",
    "\n",
    "    return {\n",
    "        'precision': (precision_mean, precision_ci_low, precision_ci_high),\n",
    "        'recall': (recall_mean, recall_ci_low, recall_ci_high),\n",
    "        'ndcg': (ndcg_mean, ndcg_ci_low, ndcg_ci_high)\n",
    "    }\n",
    "\n",
    "# Evaluate both models\n",
    "cbf_results = metrics_with_ci(preds_cbf, test_df, K=10)\n",
    "hybrid_results = metrics_with_ci(preds_hybrid, test_df, K=10)\n",
    "\n",
    "# Display results\n",
    "print(\"Model | Metric | Mean | CI-Lower | CI-Upper\")\n",
    "print(\"-\"*50)\n",
    "for model, result in zip(['CBF', 'Hybrid'], [cbf_results, hybrid_results]):\n",
    "    for metric, (mean, ci_low, ci_high) in result.items():\n",
    "        print(f\"{model:<6} | {metric:<9} | {mean:.4f} | {ci_low:.4f} | {ci_high:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
