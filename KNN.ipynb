{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ffe8c0",
   "metadata": {},
   "source": [
    "# K‑Nearest Neighbors (KNN) Collaborative Filtering\n",
    "\n",
    "- Load and validate the **subset_ratings.csv** file  \n",
    "- Pivot into a **user × movie** ratings matrix (fill NA with 0)  \n",
    "- Compute **item–item cosine similarity** and keep the top‑k neighbors  \n",
    "- Predict unknown ratings by a **mean‑centered, similarity‑weighted average**  \n",
    "- Split data per user (80 % train / 20 % test) and report **RMSE / MAE**  \n",
    "- Output **Top‑N recommendations** for 1 000 sampled users to JSON  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167a73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, warnings\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.metrics import pairwise_distances, mean_squared_error, mean_absolute_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7695882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (2078625, 3)\n"
     ]
    }
   ],
   "source": [
    "RATING_FILE = Path(\"subset_ratings.csv\")\n",
    "if not RATING_FILE.exists():\n",
    "    raise FileNotFoundError(RATING_FILE)\n",
    "\n",
    "ratings = pd.read_csv(RATING_FILE, usecols=[\"userId\", \"movieId\", \"rating\"])\n",
    "print(\"Loaded:\", ratings.shape)\n",
    "\n",
    "# map raw IDs → dense indices\n",
    "uid2idx = {u: i for i, u in enumerate(ratings.userId.unique())}\n",
    "mid2idx = {m: i for i, m in enumerate(ratings.movieId.unique())}\n",
    "idx2mid = {i: m for m, i in mid2idx.items()}\n",
    "\n",
    "ratings[\"u_idx\"] = ratings.userId.map(uid2idx)\n",
    "ratings[\"m_idx\"] = ratings.movieId.map(mid2idx)\n",
    "\n",
    "n_users, n_items = len(uid2idx), len(mid2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88b32f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 1666865   Test rows: 411760\n"
     ]
    }
   ],
   "source": [
    "def split_per_user(df, frac=0.2, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    train_mask = np.zeros(len(df), dtype=bool)\n",
    "    for _, grp in df.groupby(\"userId\"):\n",
    "        idx = grp.index.to_numpy()\n",
    "        rng.shuffle(idx)\n",
    "        k = max(1, int(len(idx) * frac))\n",
    "        train_mask[idx[k:]] = True\n",
    "    return df[train_mask], df[~train_mask]\n",
    "\n",
    "train_df, test_df = split_per_user(ratings, 0.20)\n",
    "print(f\"Train rows: {len(train_df)}   Test rows: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009352a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = csr_matrix(\n",
    "    (train_df.rating.values,\n",
    "     (train_df.m_idx, train_df.u_idx)),\n",
    "    shape=(n_items, n_users)\n",
    ")\n",
    "\n",
    "item_sum    = np.asarray(train_mat.sum(axis=1)).ravel()\n",
    "item_counts = np.diff(train_mat.indptr)                     # nnz per item\n",
    "item_means  = np.divide(item_sum, item_counts,\n",
    "                        out=np.zeros_like(item_sum, dtype=float),\n",
    "                        where=item_counts != 0)\n",
    "\n",
    "# subtract mean from each non-zero rating\n",
    "train_mc = train_mat.copy().astype(np.float32)\n",
    "for i in range(n_items):\n",
    "    start, end = train_mc.indptr[i], train_mc.indptr[i + 1]\n",
    "    train_mc.data[start:end] -= item_means[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28685bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sum    = np.asarray(train_mat.sum(axis=1)).ravel()\n",
    "item_counts = np.diff(train_mat.indptr)           # nnz per row\n",
    "item_means  = np.divide(item_sum, item_counts, out=np.zeros_like(item_sum), where=item_counts!=0)\n",
    "\n",
    "# subtract mean from each non-zero element\n",
    "train_mc = train_mat.copy().astype(np.float32)\n",
    "for i in range(n_items):\n",
    "    start, end = train_mc.indptr[i], train_mc.indptr[i+1]\n",
    "    train_mc.data[start:end] -= item_means[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f9ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity …\n"
     ]
    }
   ],
   "source": [
    "K = 40\n",
    "print(\"Computing cosine similarity …\")\n",
    "cosine_dist = pairwise_distances(train_mc, metric=\"cosine\", n_jobs=-1)\n",
    "cosine_sim  = 1.0 - cosine_dist\n",
    "np.fill_diagonal(cosine_sim, 0.0)\n",
    "\n",
    "sim_mat = lil_matrix((n_items, n_items), dtype=np.float32)\n",
    "for i in range(n_items):\n",
    "    if K >= n_items:\n",
    "        nbrs = np.argsort(cosine_sim[i])[::-1]\n",
    "    else:\n",
    "        nbrs = np.argpartition(cosine_sim[i], -K)[-K:]\n",
    "        nbrs = nbrs[np.argsort(cosine_sim[i][nbrs])[::-1]]\n",
    "    vals = cosine_sim[i, nbrs]\n",
    "    mask = vals > 0\n",
    "    if mask.any():\n",
    "        sim_mat.rows[i]  = nbrs[mask].tolist()\n",
    "        sim_mat.data[i]  = vals[mask].tolist()\n",
    "\n",
    "sim_mat = sim_mat.tocsr()\n",
    "sim_abs = sim_mat.copy();  sim_abs.data = np.abs(sim_abs.data)\n",
    "del cosine_sim, cosine_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd77f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rated = defaultdict(dict)\n",
    "for r in train_df.itertuples():\n",
    "    user_rated[r.userId][r.m_idx] = r.rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779be4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rated = defaultdict(dict)\n",
    "for r in train_df.itertuples():\n",
    "    user_rated[r.userId][r.m_idx] = r.rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "427c1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(uid, iid):\n",
    "    \"\"\"\n",
    "    Mean-centred, similarity-weighted prediction for one (user, item).\n",
    "    Uses O(K) operations.\n",
    "    \"\"\"\n",
    "    numer = denom = 0.0\n",
    "    rated = user_rated[uid]          # dict\n",
    "    start, end = sim_mat.indptr[iid], sim_mat.indptr[iid + 1]\n",
    "    for nbr_idx, sim in zip(sim_mat.indices[start:end], sim_mat.data[start:end]):\n",
    "        if nbr_idx not in rated or sim <= 0:\n",
    "            continue\n",
    "        numer += sim * (rated[nbr_idx] - item_means[nbr_idx])\n",
    "        denom += abs(sim)\n",
    "    return item_means[iid] if denom == 0 else item_means[iid] + numer / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7cb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_for_user(uid, n=10):\n",
    "    \"\"\"\n",
    "    Vectorised Top-N recommendation for a user using sparse mat-vec.\n",
    "    \"\"\"\n",
    "    rated_dict = user_rated[uid]\n",
    "    if not rated_dict:\n",
    "        return []\n",
    "\n",
    "    # sparse centred rating vector b (size n_items × 1)\n",
    "    idxs = np.fromiter(rated_dict.keys(), dtype=int)\n",
    "    vals = np.fromiter((r - item_means[i] for i, r in rated_dict.items()),\n",
    "                       dtype=np.float32)\n",
    "    b = csr_matrix((vals, (idxs, np.zeros_like(idxs))), shape=(n_items, 1))\n",
    "\n",
    "    # similarity-weighted sums\n",
    "    numer = (sim_mat @ b).toarray().ravel()\n",
    "    denom = (sim_abs @ (b != 0)).toarray().ravel()\n",
    "\n",
    "    scores = item_means + np.divide(numer, denom, out=np.zeros_like(numer),\n",
    "                                    where=denom != 0)\n",
    "    scores[idxs] = -np.inf                                # filter seen items\n",
    "    top_idx = np.argpartition(scores, -n)[-n:][np.argsort(scores[np.argpartition(scores, -n)[-n:]])][::-1]\n",
    "    return [idx2mid[int(i)] for i in top_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "645cd398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.8401   MAE = 0.6312\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = [], []\n",
    "for row in test_df.itertuples():\n",
    "    y_true.append(row.rating)\n",
    "    y_pred.append(predict_single(row.userId, row.m_idx))\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"RMSE = {rmse:.4f}   MAE = {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82936dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendations …\n",
      "✓ Saved 1000 users × 10 recs → predictions\\knn_top10_subset.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_USERS, TOP_N = 1_000, 10\n",
    "sample_users = random.sample(\n",
    "    train_df.userId.unique().tolist(),\n",
    "    min(N_USERS, train_df.userId.nunique())\n",
    ")\n",
    "\n",
    "print(\"Generating recommendations …\")\n",
    "recs = {                                          # NumPy IDs may sneak in here\n",
    "    int(u): top_n_for_user(u, TOP_N)              # keys cast to native int\n",
    "    for u in sample_users\n",
    "}\n",
    "\n",
    "# Cast every movieId to a plain Python int so json can handle it\n",
    "recs_py = {\n",
    "    uid: [int(mid) for mid in mids]               # values cast to native int\n",
    "    for uid, mids in recs.items()\n",
    "}\n",
    "\n",
    "PRED_DIR = Path(\"predictions\")\n",
    "PRED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUTFILE = PRED_DIR / \"knn_top10_subset.json\"\n",
    "with open(OUTFILE, \"w\") as f:\n",
    "    json.dump(recs_py, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved {len(recs_py)} users × {TOP_N} recs → {OUTFILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3a36a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ KNN scenario: STANDARD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KNN preds: 100%|██████████| 10000/10000 [00:15<00:00, 652.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → saved 10000 users → coldstart_pred\\knn_standard_top10.json\n",
      "\n",
      "→ KNN scenario: USER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KNN preds: 100%|██████████| 500/500 [00:00<00:00, 660.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → saved 500 users → coldstart_pred\\knn_user_top10.json\n",
      "\n",
      "→ KNN scenario: ITEM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KNN preds: 100%|██████████| 7982/7982 [00:12<00:00, 649.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → saved 7982 users → coldstart_pred\\knn_item_top10.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# assumes you already have:\n",
    "#   – predict_single(uid, iid)  \n",
    "#   – top_n_for_user(uid, n)  \n",
    "#   – split_per_user(df, frac, seed)\n",
    "#   – ratings, uid2idx, mid2idx, idx2mid\n",
    "\n",
    "def generate_knn_preds(train_df, test_df, K=10):\n",
    "    # build your train_mat, item_means, sim_mat, sim_abs, user_rated inside here,\n",
    "    # exactly as in your existing code block above before “# RMSE/MAE”...\n",
    "    #\n",
    "    # then simply:\n",
    "    recs = {}\n",
    "    for u in tqdm(test_df.userId.unique(), desc=\"KNN preds\"):\n",
    "        recs[int(u)] = top_n_for_user(u, K)\n",
    "    return recs\n",
    "\n",
    "# make sure this directory exists\n",
    "out_dir = Path(\"coldstart_pred\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for scenario in (\"STANDARD\", \"USER\", \"ITEM\"):\n",
    "    print(\"→ KNN scenario:\", scenario)\n",
    "    # 1) load splits\n",
    "    if scenario == \"STANDARD\":\n",
    "        all_r = ratings  # your full subset_ratings.csv loaded earlier\n",
    "        train_df, test_df = split_per_user(all_r, frac=0.2, seed=42)\n",
    "\n",
    "    elif scenario == \"USER\":\n",
    "        train_df = pd.read_csv(\"evaluation/user_cold_train.csv\", usecols=[\"userId\",\"movieId\",\"rating\"])\n",
    "        test_df  = pd.read_csv(\"evaluation/user_cold_test.csv\",  usecols=[\"userId\",\"movieId\",\"rating\"])\n",
    "\n",
    "    else:  # ITEM\n",
    "        train_df = pd.read_csv(\"evaluation/item_cold_train.csv\", usecols=[\"userId\",\"movieId\",\"rating\"])\n",
    "        test_df  = pd.read_csv(\"evaluation/item_cold_test.csv\",  usecols=[\"userId\",\"movieId\",\"rating\"])\n",
    "\n",
    "    # 2) filter to known items (if you do the same in your other scripts)\n",
    "    valid = set(mid2idx.keys())\n",
    "    train_df = train_df[train_df.movieId.isin(valid)].reset_index(drop=True)\n",
    "    test_df  = test_df [test_df .movieId.isin(valid)].reset_index(drop=True)\n",
    "\n",
    "    # 3) rebuild all of KNN’s internal data: train_mat, item_means, sim_mat, sim_abs, user_rated\n",
    "    #    (just copy‐paste the corresponding blocks from above)\n",
    "    #\n",
    "    #    e.g.:\n",
    "    #    train_mat = csr_matrix(...)\n",
    "    #    compute item_means, train_mc, cosine_sim, sim_mat, sim_abs, user_rated\n",
    "\n",
    "    # 4) generate & dump\n",
    "    preds = generate_knn_preds(train_df, test_df, K=10)\n",
    "    # convert both keys and all movie‐ids to native Python ints\n",
    "    clean_preds = {\n",
    "        int(u): [int(m) for m in mids]\n",
    "        for u, mids in preds.items()\n",
    "    }\n",
    "\n",
    "    outfn = out_dir / f\"knn_{scenario.lower()}_top10.json\"\n",
    "    with open(outfn, \"w\") as fp:\n",
    "        json.dump(clean_preds, fp, indent=2)\n",
    "\n",
    "    print(f\"  → saved {len(preds)} users → {outfn}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16c15d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
